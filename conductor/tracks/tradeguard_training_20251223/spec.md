# Track Specification: TradeGuard RL Training Environment

## Overview
Implement a Reinforcement Learning (RL) training pipeline for the TradeGuard model. TradeGuard acts as a secondary filtering layer that decides whether to "Allow" or "Block" trade signals generated by the Alpha model. The goal is to maximize total portfolio profit by filtering out high-risk trades while minimizing missed opportunities.

## Functional Requirements

### 1. TradeGuard Training Environment (`trade_guard_env.py`)
- **Action Space:** Discrete(2) [0: Block, 1: Allow].
- **Observation Space:** Derived from the `TradeGuardFeatureCalculator` (approx 60 features).
- **Dataset Integration:**
    - Load the ~2M row dataset generated by `generate_dataset.py`.
    - Use the entire dataset (2016-2024) for training.
    - Iterate through trades sequentially to simulate the decision-making process.
- **Reward Function (Hybrid PnL + Penalties):**
    - **Action: Allow (1)**
        - Reward = Actual Trade PnL (from the dataset).
    - **Action: Block (0)**
        - If the trade was a **Win**: Reward = -(Trade PnL) * 0.5 (Penalty for missed opportunity).
        - If the trade was a **Loss**: Reward = +(Potential Loss avoided) * 0.1 (Small incentive for a "good save").

### 2. Hyperparameter Management
- Create `TradeGuard/config/ppo_config.yaml` to store PPO model parameters (learning rate, batch size, ent_coef, etc.) and environment-specific settings.

### 3. Training Script (`train_guard.py`)
- Load configuration from YAML.
- Initialize the `TradeGuardEnv` with the full dataset.
- Train a PPO agent using `stable-baselines3`.
- Save the trained model and training logs (TensorBoard).

## Non-Functional Requirements
- **Performance:** Efficient loading and iteration over large Parquet datasets (~2M rows).
- **Reproducibility:** Use fixed seeds for environment and model initialization.

## Acceptance Criteria
- [ ] `TradeGuard/config/ppo_config.yaml` created with sensible defaults.
- [ ] `trade_guard_env.py` correctly implements the RL environment and reward logic.
- [ ] `train_guard.py` successfully trains a PPO model and saves the artifact.
- [ ] The agent shows a positive reward trend in TensorBoard.

## Out of Scope
- Modifying `generate_dataset.py`.
- Hold-out validation (Backtesting will be performed separately on 2025 data).
