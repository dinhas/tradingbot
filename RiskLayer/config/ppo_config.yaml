# =========================================================================
# RISK MODEL CONFIGURATION (TradeGuard) - v2 BALANCED
# =========================================================================

# Training Settings
total_timesteps: 10000000  # 10M steps for thorough learning

# PPO Hyperparameters (Optimized for decision-making)
learning_rate: 0.0001      # Lower LR for stability (was 0.0003)
n_steps: 4096              # Longer rollouts for better credit assignment (was 2048)
batch_size: 256            # Larger batches for stable gradients (was 64)
n_epochs: 10               # Keep same
gamma: 0.995               # Higher gamma for long-term thinking (was 0.99)
gae_lambda: 0.98           # Smoother advantage estimation (was 0.95)
clip_range: 0.15           # Tighter clipping for stability (was 0.2)
ent_coef: 0.005            # Lower entropy for more exploitation (was 0.01)
max_grad_norm: 0.5         # Keep same

# Network Architecture (Larger for 83 features)
policy_kwargs:
  net_arch:
    pi: [256, 128, 64]     # Policy network (was [64, 64])
    vf: [256, 128, 64]     # Value network (was [64, 64])
  activation_fn: "Tanh"

# Environment Settings
execution_threshold: 0.2   # Keep same
min_rr: 1.5               # Minimum Risk:Reward ratio
atr_sl_min: 1.0
atr_sl_max: 5.0
atr_tp_min: 2.0
atr_tp_max: 20.0

# =========================================================================
# REWARD SETTINGS (v2 BALANCED - Implemented in risk_env.py)
# =========================================================================
# SKIP Rewards:
#   - Dodged loser:  min(2.0, 0.5 * |R|)  [Capped]
#   - Missed winner: -2.0 * R             [Strong penalty]
# 
# OPEN Rewards:
#   - Won:  +R + 1.0  [Activity bonus]
#   - Lost: -R - 0.5  [Manageable penalty]
#
# Structure Bonus:
#   - RR >= 2.0: +0.3
#   - RR < 1.5:  -0.5
#
# Skip Rate Penalty:
#   - >70%: -1.0
#   - >80%: -3.0
# =========================================================================
