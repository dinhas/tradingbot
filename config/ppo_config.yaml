default:
  learning_rate: 0.0003
  n_steps: 4096 # INCREASED: From 2048 to 4096 for better samples per update
  batch_size: 512 # INCREASED: From 64 to 512 for T4 GPU speedup
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.15  # REDUCED: From 0.2 to 0.15 for more stable updates with penalty-heavy rewards
  vf_coef: 0.5
  max_grad_norm: 0.5
  verbose: 1
  tensorboard_log: "./logs/tensorboard"
  # Default total_timesteps if not specified in args
  total_timesteps: 3000000  # INCREASED: From 1.5M to 3M for more learning
  policy_kwargs:
    net_arch: [256, 256, 128]
    activation_fn: "ReLU"

stages:
  1:
    # PENALTY FEAR UPDATE (Dec 9): Reduced from 0.05 to 0.02
    # With loss aversion & stronger penalties, we need LESS exploration
    # Too much entropy = random trades = get punished hard by new penalties
    ent_coef: 0.02
  2:
    ent_coef: 0.01
  3:
    ent_coef: 0.005
    policy_kwargs:
      net_arch: [256, 256, 256] # Larger network for 20 outputs
      activation_fn: "LeakyReLU"
