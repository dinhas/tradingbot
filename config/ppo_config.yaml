default:
  # =========================================================================
  # EFFICIENCY OPTIMIZED FOR 1.5M STEPS (Dec 9)
  # Goal: Maximum learning per sample without overfitting
  # NO linear schedules - constant values throughout training
  # =========================================================================
  
  learning_rate: 0.0005  # INCREASED: From 0.0003 - faster learning (no schedule!)
  
  # MORE FREQUENT UPDATES
  n_steps: 2048         # HALVED: From 4096 - 2x more policy updates (732 vs 366)
  
  # MORE GRADIENT STEPS PER UPDATE  
  batch_size: 256       # HALVED: From 512 - 2x more batches per epoch
  n_epochs: 15          # INCREASED: From 10 - 1.5x more epochs per update
  
  # Net effect: (2048/256) * 15 = 120 gradient steps per 2048 samples
  # vs old:     (4096/512) * 10 = 80 gradient steps per 4096 samples
  # = 3x more gradient updates per sample!
  
  # LONGER HORIZON THINKING -> CHANGED TO GREEDIER (Dec 9)
  gamma: 0.99           # REDUCED: From 0.995 - value IMMEDIATE rewards more (greedier)
  gae_lambda: 0.98      # INCREASED: From 0.97 - trust value function more (chase trends)
  
  # STABILITY
  clip_range: 0.15      # Conservative policy updates
  vf_coef: 0.75         # INCREASED: From 0.5 - stronger value function learning
  max_grad_norm: 0.3    # TIGHTER: From 0.5 - prevent gradient explosions
  
  verbose: 1
  tensorboard_log: "./logs/tensorboard"
  total_timesteps: 1500000  # Target: 1.5M steps
  
  policy_kwargs:
    net_arch: [256, 256, 128]
    activation_fn: "ReLU"

stages:
  1:
    # PENALTY FEAR + EFFICIENCY UPDATE (Dec 9)
    # Lower entropy = less random exploration = more exploitation of learned patterns
    # With stronger penalties, random trades get punished hard
    ent_coef: 0.015     # REDUCED: From 0.02 - even less randomness for efficiency
  2:
    ent_coef: 0.01
  3:
    ent_coef: 0.005
    policy_kwargs:
      net_arch: [256, 256, 256] # Larger network for 20 outputs
      activation_fn: "LeakyReLU"
