# =========================================================================
# STABILIZED ALPHA CONFIG â€” 1.5M Steps (Optimized for V2)
# =========================================================================

# Learning Rate: Reduced to 5e-5 for very smooth, stable updates
learning_rate: 0.00005

# Rollout Settings: 
# Large n_steps and batch_size for high-quality gradient estimation
n_steps: 4096
batch_size: 1024
n_epochs: 20

# Temporal Discounting
gamma: 0.95
gae_lambda: 0.95

# PPO Clipping: Tightened to 0.1 for incremental policy shifts
clip_range: 0.1
clip_range_vf: null

# Entropy Regularization: Lowered to 0.01 to allow convergence while preventing premature stagnation
ent_coef: 0.01

# Loss Coefficients
vf_coef: 0.5

# Gradient Clipping
max_grad_norm: 0.5

# Training Budget
total_timesteps: 2500000

# Network Architecture
# Reverted to V1 Baseline [256, 256, 128]
# While inputs are fewer (40 vs 140), the 'Peek & Label' task (predicting 1000 steps ahead) 
# requires deep non-linearity. 2.5M data points support this capacity without overfitting.
policy_kwargs:
  net_arch: [256, 256, 128]
  activation_fn: "ReLU"