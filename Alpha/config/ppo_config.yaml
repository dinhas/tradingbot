# =========================================================================
# STABILIZED ALPHA CONFIG â€” 1.5M Steps
# =========================================================================

# Learning Rate: Reduced for more stable updates and to prevent divergence
learning_rate: 0.0001

# Rollout Settings: 
# Increased n_steps to 2048 to provide more stable gradient estimates
n_steps: 2048
batch_size: 512
n_epochs: 10

# Temporal Discounting
# Increased gamma to 0.99 for a more balanced long-term perspective
gamma: 0.99
gae_lambda: 0.95

# PPO Clipping
clip_range: 0.2
clip_range_vf: null

# Entropy Regularization: Reduced slightly to 0.005 to reduce noise 
# while still encouraging exploration.
ent_coef: 0.005

# Loss Coefficients
vf_coef: 0.5

# Gradient Clipping
max_grad_norm: 0.5

# Training Budget
total_timesteps: 1500000

# Network Architecture
policy_kwargs:
  net_arch: [192, 192, 96]
  activation_fn: "ReLU"